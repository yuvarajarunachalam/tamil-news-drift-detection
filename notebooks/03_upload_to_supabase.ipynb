{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7330a11a-6759-40ca-a304-0607e6eb7c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Supabase client initialized!\n",
      "Connected to: https://lgnhjzlbezpczlobeevu.supabase.co\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from supabase import create_client, Client\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# ============================================\n",
    "# SUPABASE CREDENTIALS - REPLACE THESE\n",
    "# ============================================\n",
    "SUPABASE_URL = \"https://lgnhjzlbezpczlobeevu.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImxnbmhqemxiZXpwY3psb2JlZXZ1Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTgyMDgzNjcsImV4cCI6MjA3Mzc4NDM2N30.O5Yt0dOyYq326ESo0LBL7lGj4k8zwpuodJfTtGwrPek\"\n",
    "# ============================================\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "print(\"‚úÖ Supabase client initialized!\")\n",
    "print(f\"Connected to: {SUPABASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ae901b-03ea-48b5-846b-fc32c0dc696f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Table doesn't exist yet (expected): APIError\n",
      "‚û°Ô∏è Continue to next cell to create table\n"
     ]
    }
   ],
   "source": [
    "# Test connection by querying (will fail if table doesn't exist yet - that's expected)\n",
    "try:\n",
    "    result = supabase.table('news_raw').select(\"*\").limit(1).execute()\n",
    "    print(f\"‚úÖ Connection successful! Found {len(result.data)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Table doesn't exist yet (expected): {type(e).__name__}\")\n",
    "    print(\"‚û°Ô∏è Continue to next cell to create table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646e829c-09d9-4826-aa34-35b16d18984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã SQL Schema prepared\n",
      "\n",
      "‚ö†Ô∏è NOTE: Run this SQL manually in Supabase SQL Editor:\n",
      "   1. Go to Supabase Dashboard ‚Üí SQL Editor\n",
      "   2. Click 'New Query'\n",
      "   3. Copy-paste the SQL below\n",
      "   4. Click 'Run'\n",
      "\n",
      "============================================================\n",
      "\n",
      "CREATE TABLE IF NOT EXISTS news_raw (\n",
      "  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
      "  \n",
      "  -- Core Fields\n",
      "  title TEXT NOT NULL,\n",
      "  link TEXT NOT NULL UNIQUE,\n",
      "  category TEXT,\n",
      "  source TEXT NOT NULL,\n",
      "  \n",
      "  -- Date Fields\n",
      "  pub_date TIMESTAMPTZ,\n",
      "  archive_date DATE,\n",
      "  scraped_at TIMESTAMPTZ DEFAULT NOW(),\n",
      "  \n",
      "  -- Content Fields\n",
      "  description TEXT,\n",
      "  content_full TEXT,\n",
      "  \n",
      "  -- Metadata\n",
      "  guid TEXT,\n",
      "  image_url TEXT,\n",
      "  image_width INTEGER,\n",
      "  image_height INTEGER,\n",
      "  \n",
      "  -- Flags\n",
      "  has_description BOOLEAN DEFAULT FALSE,\n",
      "  has_image BOOLEAN DEFAULT FALSE,\n",
      "  needs_full_scrape BOOLEAN DEFAULT FALSE,\n",
      "  \n",
      "  -- Raw Data\n",
      "  raw_json JSONB,\n",
      "  \n",
      "  -- Constraint\n",
      "  CONSTRAINT check_has_date CHECK (pub_date IS NOT NULL OR archive_date IS NOT NULL)\n",
      ");\n",
      "\n",
      "-- Create indexes\n",
      "CREATE INDEX IF NOT EXISTS idx_news_raw_pub_date ON news_raw(pub_date);\n",
      "CREATE INDEX IF NOT EXISTS idx_news_raw_archive_date ON news_raw(archive_date);\n",
      "CREATE INDEX IF NOT EXISTS idx_news_raw_category ON news_raw(category);\n",
      "CREATE INDEX IF NOT EXISTS idx_news_raw_source ON news_raw(source);\n",
      "CREATE INDEX IF NOT EXISTS idx_news_raw_link ON news_raw(link);\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# SQL to create the table\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS news_raw (\n",
    "  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
    "  \n",
    "  -- Core Fields\n",
    "  title TEXT NOT NULL,\n",
    "  link TEXT NOT NULL UNIQUE,\n",
    "  category TEXT,\n",
    "  source TEXT NOT NULL,\n",
    "  \n",
    "  -- Date Fields\n",
    "  pub_date TIMESTAMPTZ,\n",
    "  archive_date DATE,\n",
    "  scraped_at TIMESTAMPTZ DEFAULT NOW(),\n",
    "  \n",
    "  -- Content Fields\n",
    "  description TEXT,\n",
    "  content_full TEXT,\n",
    "  \n",
    "  -- Metadata\n",
    "  guid TEXT,\n",
    "  image_url TEXT,\n",
    "  image_width INTEGER,\n",
    "  image_height INTEGER,\n",
    "  \n",
    "  -- Flags\n",
    "  has_description BOOLEAN DEFAULT FALSE,\n",
    "  has_image BOOLEAN DEFAULT FALSE,\n",
    "  needs_full_scrape BOOLEAN DEFAULT FALSE,\n",
    "  \n",
    "  -- Raw Data\n",
    "  raw_json JSONB,\n",
    "  \n",
    "  -- Constraint\n",
    "  CONSTRAINT check_has_date CHECK (pub_date IS NOT NULL OR archive_date IS NOT NULL)\n",
    ");\n",
    "\n",
    "-- Create indexes\n",
    "CREATE INDEX IF NOT EXISTS idx_news_raw_pub_date ON news_raw(pub_date);\n",
    "CREATE INDEX IF NOT EXISTS idx_news_raw_archive_date ON news_raw(archive_date);\n",
    "CREATE INDEX IF NOT EXISTS idx_news_raw_category ON news_raw(category);\n",
    "CREATE INDEX IF NOT EXISTS idx_news_raw_source ON news_raw(source);\n",
    "CREATE INDEX IF NOT EXISTS idx_news_raw_link ON news_raw(link);\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã SQL Schema prepared\")\n",
    "print(\"\\n‚ö†Ô∏è NOTE: Run this SQL manually in Supabase SQL Editor:\")\n",
    "print(\"   1. Go to Supabase Dashboard ‚Üí SQL Editor\")\n",
    "print(\"   2. Click 'New Query'\")\n",
    "print(\"   3. Copy-paste the SQL below\")\n",
    "print(\"   4. Click 'Run'\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(create_table_sql)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3899232f-38bd-4719-b886-31a7b79ee9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table 'news_raw' exists!\n",
      "Current row count: 0\n"
     ]
    }
   ],
   "source": [
    "# Test that table now exists\n",
    "try:\n",
    "    result = supabase.table('news_raw').select(\"*\").limit(1).execute()\n",
    "    print(f\"‚úÖ Table 'news_raw' exists!\")\n",
    "    print(f\"Current row count: {len(result.data)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e91b80-4e9c-4370-97da-d94af0162759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DataFrames loaded:\n",
      "RSS file: hindu_rss_20251027_125409.csv ‚Äî 100 rows\n",
      "Archive file: archive_sept27_tn_20251027_132501.csv ‚Äî 61 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ Define base directory\n",
    "base_dir = r\"C:\\Users\\Yuvaraj\\Desktop\\Data-Science\\Mini-project\\tamil-news-drift-detection\\data\\raw\"\n",
    "\n",
    "# ‚úÖ Find the most recent matching files\n",
    "rss_files = glob.glob(os.path.join(base_dir, \"hindu_rss_*.csv\"))\n",
    "archive_files = glob.glob(os.path.join(base_dir, \"archive_sept27_tn_*.csv\"))\n",
    "\n",
    "# ‚úÖ Get the newest ones (based on modification time)\n",
    "latest_rss = max(rss_files, key=os.path.getmtime) if rss_files else None\n",
    "latest_archive = max(archive_files, key=os.path.getmtime) if archive_files else None\n",
    "\n",
    "# ‚úÖ Load data if found\n",
    "if latest_rss and latest_archive:\n",
    "    df_rss = pd.read_csv(latest_rss)\n",
    "    df_archive = pd.read_csv(latest_archive)\n",
    "\n",
    "    print(\"üìä DataFrames loaded:\")\n",
    "    print(f\"RSS file: {os.path.basename(latest_rss)} ‚Äî {len(df_rss)} rows\")\n",
    "    print(f\"Archive file: {os.path.basename(latest_archive)} ‚Äî {len(df_archive)} rows\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No matching CSV files found. Check your 'data/raw' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7b5744-ff35-4458-a112-c39b707efddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformed 100 RSS records\n",
      "\n",
      "üîç Data validation:\n",
      "Records with image_width: 77\n",
      "Records with image_height: 77\n",
      "\n",
      "Sample record:\n",
      "{\n",
      "  \"title\": \"Orange alert issued in north Tamil Nadu districts as Cyclone Montha advances\",\n",
      "  \"link\": \"https://www.thehindu.com/news/national/tamil-nadu/cyclone-montha-orange-alert-issued-in-north-tamil-nadu-districts-as-weather-system-advances/article70207325.ece\",\n",
      "  \"category\": \"Tamil Nadu\",\n",
      "  \"source\": \"The Hindu - RSS\",\n",
      "  \"pub_date\": \"2025-10-27 12:23:45\",\n",
      "  \"archive_date\": null,\n",
      "  \"description\": \"In its Nowcast till 1 p.m. on Monday (October 27), the RMC has predicted moderate rains to continue over Chennai and its neighbouring districts, and Villupuram and Ranipet\",\n",
      "  \"content_full\": null,\n",
      "  \"guid\": \"article-70207325\",\n",
      "  \"image_url\": \"https://th-i.thgim.com/public/incoming/z04cxf/article70207451.ece/alternates/LANDSCAPE_1200/2315_25_10_2025_16_55_13_2_CLOUDS2.JPG\",\n",
      "  \"image_width\": 1200,\n",
      "  \"image_height\": 675,\n",
      "  \"has_description\": true,\n",
      "  \"has_image\": true,\n",
      "  \"needs_full_scrape\": false,\n",
      "  \"raw_json\": null,\n",
      "  \"scraped_at\": \"2025-10-27 12:50:38\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def transform_rss_for_upload(df_rss):\n",
    "    \"\"\"\n",
    "    Transform RSS DataFrame to match Supabase schema with proper data cleaning\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for _, row in df_rss.iterrows():\n",
    "        # Helper function to safely convert to int\n",
    "        def safe_int(value):\n",
    "            try:\n",
    "                if pd.isna(value) or value == '' or value == 'None':\n",
    "                    return None\n",
    "                return int(float(value))\n",
    "            except (ValueError, TypeError):\n",
    "                return None\n",
    "        \n",
    "        # Helper function to safely get string value\n",
    "        def safe_str(value):\n",
    "            if pd.isna(value) or value == '' or value == 'None':\n",
    "                return None\n",
    "            return str(value).strip()\n",
    "        \n",
    "        record = {\n",
    "            'title': str(row['title']).strip(),\n",
    "            'link': str(row['link']).strip(),\n",
    "            'category': safe_str(row['category']),\n",
    "            'source': 'The Hindu - RSS',\n",
    "            'pub_date': str(row['pub_date']),\n",
    "            'archive_date': None,\n",
    "            'description': safe_str(row['description']),\n",
    "            'content_full': None,\n",
    "            'guid': safe_str(row['guid']),\n",
    "            'image_url': safe_str(row['image_url']),\n",
    "            'image_width': safe_int(row['image_width']),\n",
    "            'image_height': safe_int(row['image_height']),\n",
    "            'has_description': bool(row['has_description']),\n",
    "            'has_image': bool(row['has_image']),\n",
    "            'needs_full_scrape': not bool(row['has_description']),\n",
    "            'raw_json': None,\n",
    "            'scraped_at': str(row['scraped_at'])\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Transform\n",
    "rss_records = transform_rss_for_upload(df_rss)\n",
    "print(f\"‚úÖ Transformed {len(rss_records)} RSS records\")\n",
    "\n",
    "# Check for invalid values\n",
    "print(\"\\nüîç Data validation:\")\n",
    "print(f\"Records with image_width: {sum(1 for r in rss_records if r['image_width'] is not None)}\")\n",
    "print(f\"Records with image_height: {sum(1 for r in rss_records if r['image_height'] is not None)}\")\n",
    "\n",
    "print(f\"\\nSample record:\")\n",
    "print(json.dumps(rss_records[0], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c93fa94d-7a5c-4db9-972c-e6f2fe0e3a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformed 61 Archive records\n",
      "\n",
      "Sample record:\n",
      "{\n",
      "  \"title\": \"Encroachments demolished to complete new bridge across Kamadalam river near Arani\",\n",
      "  \"link\": \"https://www.thehindu.com/news/national/tamil-nadu/encroachments-demolished-to-complete-new-bridge-across-kamadalam-river-near-arani/article70102100.ece\",\n",
      "  \"category\": \"Tamil Nadu\",\n",
      "  \"source\": \"The Hindu - Archive\",\n",
      "  \"pub_date\": null,\n",
      "  \"archive_date\": \"2025-09-27\",\n",
      "  \"description\": null,\n",
      "  \"content_full\": null,\n",
      "  \"guid\": null,\n",
      "  \"image_url\": null,\n",
      "  \"image_width\": null,\n",
      "  \"image_height\": null,\n",
      "  \"has_description\": false,\n",
      "  \"has_image\": false,\n",
      "  \"needs_full_scrape\": true,\n",
      "  \"raw_json\": null,\n",
      "  \"scraped_at\": \"2025-10-27 13:24:57\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def transform_archive_for_upload(df_archive):\n",
    "    \"\"\"\n",
    "    Transform Archive DataFrame to match Supabase schema\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for _, row in df_archive.iterrows():\n",
    "        record = {\n",
    "            'title': row['title'],\n",
    "            'link': row['link'],\n",
    "            'category': row['category'],\n",
    "            'source': 'The Hindu - Archive',\n",
    "            'pub_date': None,\n",
    "            'archive_date': row['archive_date'],  # Format: YYYY-MM-DD\n",
    "            'description': None,\n",
    "            'content_full': None,\n",
    "            'guid': None,\n",
    "            'image_url': None,\n",
    "            'image_width': None,\n",
    "            'image_height': None,\n",
    "            'has_description': False,\n",
    "            'has_image': False,\n",
    "            'needs_full_scrape': True,  # Archives always need full scraping\n",
    "            'raw_json': None,\n",
    "            'scraped_at': row['scraped_at']\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Transform\n",
    "archive_records = transform_archive_for_upload(df_archive)\n",
    "print(f\"‚úÖ Transformed {len(archive_records)} Archive records\")\n",
    "print(f\"\\nSample record:\")\n",
    "print(json.dumps(archive_records[0], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ed0be5f-c65b-4369-bf72-f6e2d27cd2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting RSS upload...\n",
      "============================================================\n",
      "üì§ Uploading batch 1/3 (50 records)... ‚úÖ Success\n",
      "üì§ Uploading batch 2/3 (50 records)... ‚úÖ Success\n",
      "============================================================\n",
      "‚úÖ Upload Complete!\n",
      "   Uploaded: 100\n",
      "   Duplicates: 0\n",
      "   Errors: 0\n"
     ]
    }
   ],
   "source": [
    "def upload_records_batch(records, batch_size=50):\n",
    "    \"\"\"\n",
    "    Upload records in batches with error handling\n",
    "    \"\"\"\n",
    "    total = len(records)\n",
    "    uploaded = 0\n",
    "    duplicates = 0\n",
    "    errors = 0\n",
    "    \n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = records[i:i+batch_size]\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        total_batches = (total // batch_size) + 1\n",
    "        \n",
    "        print(f\"üì§ Uploading batch {batch_num}/{total_batches} ({len(batch)} records)...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            result = supabase.table('news_raw').insert(batch).execute()\n",
    "            uploaded += len(batch)\n",
    "            print(f\"‚úÖ Success\")\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            if 'duplicate key value violates unique constraint' in error_msg.lower():\n",
    "                duplicates += len(batch)\n",
    "                print(f\"‚ö†Ô∏è Duplicates (skipped)\")\n",
    "            else:\n",
    "                errors += len(batch)\n",
    "                print(f\"‚ùå Error: {error_msg[:50]}\")\n",
    "    \n",
    "    return uploaded, duplicates, errors\n",
    "\n",
    "\n",
    "# Upload RSS records\n",
    "print(\"üöÄ Starting RSS upload...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "uploaded, duplicates, errors = upload_records_batch(rss_records)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Upload Complete!\")\n",
    "print(f\"   Uploaded: {uploaded}\")\n",
    "print(f\"   Duplicates: {duplicates}\")\n",
    "print(f\"   Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5b7823-3fd6-4010-a63b-c9244c467796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting Archive upload...\n",
      "============================================================\n",
      "üì§ Uploading batch 1/2 (50 records)... ‚úÖ Success\n",
      "üì§ Uploading batch 2/2 (11 records)... ‚ö†Ô∏è Duplicates (skipped)\n",
      "============================================================\n",
      "‚úÖ Upload Complete!\n",
      "   Uploaded: 50\n",
      "   Duplicates: 11\n",
      "   Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Upload Archive records\n",
    "print(\"\\nüöÄ Starting Archive upload...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "uploaded, duplicates, errors = upload_records_batch(archive_records)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Upload Complete!\")\n",
    "print(f\"   Uploaded: {uploaded}\")\n",
    "print(f\"   Duplicates: {duplicates}\")\n",
    "print(f\"   Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea46e65a-406d-4566-89b9-3a76b24577dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä DATABASE SUMMARY\n",
      "============================================================\n",
      "Total Records: 150\n",
      "\n",
      "By Source:\n",
      "  RSS: 100\n",
      "  Archive: 50\n",
      "\n",
      "Needs Full Scraping: 82\n"
     ]
    }
   ],
   "source": [
    "# Query total records\n",
    "result = supabase.table('news_raw').select(\"*\", count='exact').execute()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä DATABASE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Records: {result.count}\")\n",
    "\n",
    "# Count by source\n",
    "rss_count = supabase.table('news_raw').select(\"*\", count='exact').eq('source', 'The Hindu - RSS').execute()\n",
    "archive_count = supabase.table('news_raw').select(\"*\", count='exact').eq('source', 'The Hindu - Archive').execute()\n",
    "\n",
    "print(f\"\\nBy Source:\")\n",
    "print(f\"  RSS: {rss_count.count}\")\n",
    "print(f\"  Archive: {archive_count.count}\")\n",
    "\n",
    "# Count articles needing full scrape\n",
    "needs_scrape = supabase.table('news_raw').select(\"*\", count='exact').eq('needs_full_scrape', True).execute()\n",
    "print(f\"\\nNeeds Full Scraping: {needs_scrape.count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab6cffb9-f300-4070-9ae5-0bd2bfe7f376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì∞ SAMPLE RECORDS FROM DATABASE\n",
      "============================================================\n",
      "\n",
      "1. [The Hindu - RSS] Tamil Nadu\n",
      "   Orange alert issued in north Tamil Nadu districts as Cyclone Montha ad...\n",
      "   Date: 2025-10-27T12:23:45+00:00\n",
      "   Has Description: True\n",
      "\n",
      "2. [The Hindu - RSS] Chennai\n",
      "   Kalaignar International Convention Centre set for completion by Februa...\n",
      "   Date: 2025-10-27T11:59:34+00:00\n",
      "   Has Description: True\n",
      "\n",
      "3. [The Hindu - RSS] Tamil Nadu\n",
      "   Eminent plastic surgeon K. Mathangi Ramakrishnan no more...\n",
      "   Date: 2025-10-27T11:09:02+00:00\n",
      "   Has Description: True\n",
      "\n",
      "4. [The Hindu - RSS] Tamil Nadu\n",
      "   What is the problem faced by paddy farmers of Tamil Nadu? | Explained...\n",
      "   Date: 2025-10-27T08:30:00+00:00\n",
      "   Has Description: True\n",
      "\n",
      "5. [The Hindu - RSS] Tamil Nadu\n",
      "   BM reviews works in Adyar estuary...\n",
      "   Date: 2025-10-27T07:35:37+00:00\n",
      "   Has Description: True\n"
     ]
    }
   ],
   "source": [
    "# Fetch and display 5 random records\n",
    "result = supabase.table('news_raw').select(\"title, category, source, pub_date, archive_date, has_description\").limit(5).execute()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì∞ SAMPLE RECORDS FROM DATABASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, record in enumerate(result.data, 1):\n",
    "    print(f\"\\n{i}. [{record['source']}] {record['category']}\")\n",
    "    print(f\"   {record['title'][:70]}...\")\n",
    "    print(f\"   Date: {record.get('pub_date') or record.get('archive_date')}\")\n",
    "    print(f\"   Has Description: {record['has_description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449d3e8-917a-4e24-8da3-7e4a1afb3c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
