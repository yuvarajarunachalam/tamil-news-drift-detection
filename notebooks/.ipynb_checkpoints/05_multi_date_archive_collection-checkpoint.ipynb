{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cc6b9c9-f0d7-491a-914f-afc00610b5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# 05: Multi-Date Archive Collection\n",
    "# Goal: Scrape The Hindu archives for multiple dates (Sept 20-27, 2025)\n",
    "# Expected: ~400-500 Tamil Nadu articles across 7 days\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from supabase import create_client, Client\n",
    "import undetected_chromedriver as uc\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7583628e-f389-4548-9ac1-a683f6b5e825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ DATE RANGE: 2025-06-05 to 2025-07-05\n",
      "üìÖ Total Days: 31\n",
      "\n",
      "Dates to scrape:\n",
      "  1. 2025-06-05\n",
      "  2. 2025-06-06\n",
      "  3. 2025-06-07\n",
      "  4. 2025-06-08\n",
      "  5. 2025-06-09\n",
      "  6. 2025-06-10\n",
      "  7. 2025-06-11\n",
      "  8. 2025-06-12\n",
      "  9. 2025-06-13\n",
      "  10. 2025-06-14\n",
      "  11. 2025-06-15\n",
      "  12. 2025-06-16\n",
      "  13. 2025-06-17\n",
      "  14. 2025-06-18\n",
      "  15. 2025-06-19\n",
      "  16. 2025-06-20\n",
      "  17. 2025-06-21\n",
      "  18. 2025-06-22\n",
      "  19. 2025-06-23\n",
      "  20. 2025-06-24\n",
      "  21. 2025-06-25\n",
      "  22. 2025-06-26\n",
      "  23. 2025-06-27\n",
      "  24. 2025-06-28\n",
      "  25. 2025-06-29\n",
      "  26. 2025-06-30\n",
      "  27. 2025-07-01\n",
      "  28. 2025-07-02\n",
      "  29. 2025-07-03\n",
      "  30. 2025-07-04\n",
      "  31. 2025-07-05\n",
      "\n",
      "‚öôÔ∏è Settings:\n",
      "  Max pages to check: 15\n",
      "  Rate limit: 2s between pages\n",
      "  Rate limit: 5s between dates\n",
      "  TN categories: 26\n",
      "\n",
      "üí° Note: Actual pages per date will be detected automatically\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DATE RANGE CONFIGURATION\n",
    "# ============================================\n",
    "START_DATE = \"2025-06-05\"\n",
    "END_DATE = \"2025-07-05\"\n",
    "\n",
    "# Generate date range\n",
    "start = datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "end = datetime.strptime(END_DATE, '%Y-%m-%d')\n",
    "date_list = []\n",
    "\n",
    "current = start\n",
    "while current <= end:\n",
    "    date_list.append(current.strftime('%Y-%m-%d'))\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "print(f\"üéØ DATE RANGE: {START_DATE} to {END_DATE}\")\n",
    "print(f\"üìÖ Total Days: {len(date_list)}\")\n",
    "print(f\"\\nDates to scrape:\")\n",
    "for i, date in enumerate(date_list, 1):\n",
    "    print(f\"  {i}. {date}\")\n",
    "\n",
    "# ============================================\n",
    "# ARCHIVE SETTINGS\n",
    "# ============================================\n",
    "BASE_ARCHIVE_URL = \"https://www.thehindu.com/archive/web/\"\n",
    "MAX_PAGES_TO_CHECK = 15  # Maximum pages to check for each date\n",
    "\n",
    "# Tamil Nadu categories\n",
    "TN_CATEGORIES = {\n",
    "    'tamil nadu',\n",
    "    'chennai', 'coimbatore', 'madurai',\n",
    "    'tiruchirappalli', 'tiruchi', 'trichy',\n",
    "    'salem', 'tirunelveli', 'erode', 'vellore',\n",
    "    'thoothukudi', 'tuticorin', 'tiruppur',\n",
    "    'dindigul', 'thanjavur', 'nagercoil',\n",
    "    'kanyakumari', 'kanniyakumari', 'karur',\n",
    "    'pudukkottai', 'cuddalore', 'hosur',\n",
    "    'kanchipuram', 'avadi', 'kancheepuram'\n",
    "}\n",
    "\n",
    "# Scraping settings\n",
    "RATE_LIMIT_SECONDS = 2\n",
    "RATE_LIMIT_BETWEEN_DATES = 5\n",
    "REQUEST_TIMEOUT = 10\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Settings:\")\n",
    "print(f\"  Max pages to check: {MAX_PAGES_TO_CHECK}\")\n",
    "print(f\"  Rate limit: {RATE_LIMIT_SECONDS}s between pages\")\n",
    "print(f\"  Rate limit: {RATE_LIMIT_BETWEEN_DATES}s between dates\")\n",
    "print(f\"  TN categories: {len(TN_CATEGORIES)}\")\n",
    "print(f\"\\nüí° Note: Actual pages per date will be detected automatically\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a0e22e5-1a33-430f-a548-bc73d7284691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Supabase client initialized!\n",
      "Connected to: https://lgnhjzlbezpczlobeevu.supabase.co\n",
      "\n",
      "üìä Current articles in database: 2744\n"
     ]
    }
   ],
   "source": [
    "env_path = os.path.join(os.path.dirname(os.getcwd()), 'config', '.env')\n",
    "load_dotenv(env_path)\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "print(\"‚úÖ Supabase client initialized!\")\n",
    "print(f\"Connected to: {SUPABASE_URL}\")\n",
    "\n",
    "# Test connection - check current article count\n",
    "try:\n",
    "    result = supabase.table('news_cleaned').select('*', count='exact').execute()\n",
    "    print(f\"\\nüìä Current articles in database: {result.count}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Connection test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fedf2a6e-5831-44e7-a6f8-00d769f27e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined:\n",
      "  - parse_articles_from_html()\n",
      "  - is_tamil_nadu_article()\n"
     ]
    }
   ],
   "source": [
    "def parse_articles_from_html(html_content):\n",
    "    \"\"\"\n",
    "    Parse all articles from archive page HTML\n",
    "    Returns: List of article dictionaries\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all article elements\n",
    "    article_elements = soup.find_all('div', class_='element')\n",
    "    \n",
    "    articles = []\n",
    "    \n",
    "    for element in article_elements:\n",
    "        # Extract category label\n",
    "        label_div = element.find('div', class_='label')\n",
    "        category = label_div.get_text(strip=True) if label_div else ''\n",
    "        \n",
    "        # Extract title and link\n",
    "        title_div = element.find('div', class_='title')\n",
    "        if title_div:\n",
    "            link_tag = title_div.find('a')\n",
    "            if link_tag:\n",
    "                title = link_tag.get_text(strip=True)\n",
    "                link = link_tag.get('href', '')\n",
    "                \n",
    "                articles.append({\n",
    "                    'category': category,\n",
    "                    'title': title,\n",
    "                    'link': link\n",
    "                })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "\n",
    "def is_tamil_nadu_article(category):\n",
    "    \"\"\"\n",
    "    Check if article belongs to Tamil Nadu\n",
    "    \"\"\"\n",
    "    return category.lower() in TN_CATEGORIES\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined:\")\n",
    "print(\"  - parse_articles_from_html()\")\n",
    "print(\"  - is_tamil_nadu_article()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae5dc3ad-0a82-49dc-95f2-62e9dd98e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core scraping function defined: scrape_archive_for_date()\n"
     ]
    }
   ],
   "source": [
    "def scrape_archive_for_date(target_date, driver, total_pages=12):\n",
    "    \"\"\"\n",
    "    Scrape all pages for a specific date\n",
    "    \n",
    "    Args:\n",
    "        target_date: str in format 'YYYY-MM-DD'\n",
    "        driver: Selenium WebDriver instance\n",
    "        total_pages: int, number of pages to scrape\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame of Tamil Nadu articles for that date\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    \n",
    "    # Convert date to URL format: YYYY/MM/DD\n",
    "    date_parts = target_date.split('-')\n",
    "    url_date = f\"{date_parts[0]}/{date_parts[1]}/{date_parts[2]}\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìÖ SCRAPING DATE: {target_date}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for page_num in range(1, total_pages + 1):\n",
    "        print(f\"  üìÑ Page {page_num}/{total_pages}...\", end=\" \")\n",
    "        \n",
    "        # Construct URL\n",
    "        if page_num == 1:\n",
    "            url = f\"{BASE_ARCHIVE_URL}{url_date}/\"\n",
    "        else:\n",
    "            url = f\"{BASE_ARCHIVE_URL}{url_date}/?page={page_num}\"\n",
    "        \n",
    "        try:\n",
    "            # Load page\n",
    "            driver.get(url)\n",
    "    \n",
    "            # Wait longer for Cloudflare to resolve\n",
    "            time.sleep(8)  # Increased wait time\n",
    "            \n",
    "            # Check if we're past Cloudflare\n",
    "            print(f\"    Page title: {driver.title}\")\n",
    "            \n",
    "            # Wait for articles to load (increased timeout)\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"element\"))\n",
    "            )\n",
    "            \n",
    "            # Small delay to ensure full load\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Get page source and parse\n",
    "            html_content = driver.page_source\n",
    "            articles = parse_articles_from_html(html_content)\n",
    "            all_articles.extend(articles)\n",
    "            \n",
    "            print(f\"‚úÖ {len(articles)} articles\")\n",
    "            \n",
    "            # Rate limiting between pages\n",
    "            if page_num < total_pages:\n",
    "                time.sleep(RATE_LIMIT_SECONDS)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed - {str(e)[:1000]}...\")\n",
    "            continue\n",
    "    \n",
    "    # Filter for Tamil Nadu articles\n",
    "    tn_articles = [a for a in all_articles if is_tamil_nadu_article(a['category'])]\n",
    "    \n",
    "    # Add metadata\n",
    "    for article in tn_articles:\n",
    "        article['archive_date'] = target_date\n",
    "        article['source'] = 'The Hindu - Archive'\n",
    "        article['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        article['has_summary'] = False\n",
    "        article['needs_full_scrape'] = True\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(tn_articles)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Date complete: {len(all_articles)} total ‚Üí {len(tn_articles)} TN articles ({len(tn_articles)/len(all_articles)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"‚úÖ Core scraping function defined: scrape_archive_for_date()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "698c7422-b43b-4778-8aca-9a030a103672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_selenium_driver():\n",
    "    \"\"\"\n",
    "    Initialize Chrome driver with Cloudflare bypass settings\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    \n",
    "    # Use new headless mode (better detection avoidance)\n",
    "    # chrome_options.add_argument('--headless=new')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    # Anti-detection measures\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    # Realistic user agent (update to latest Chrome version)\n",
    "    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36')\n",
    "    \n",
    "    # Additional fingerprint masking\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    chrome_options.add_argument('--proxy-server=\"direct://\"')\n",
    "    chrome_options.add_argument('--proxy-bypass-list=*')\n",
    "    chrome_options.add_argument('--start-maximized')\n",
    "    \n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()), \n",
    "        options=chrome_options\n",
    "    )\n",
    "    \n",
    "    # Execute CDP commands to mask automation\n",
    "    driver.execute_cdp_cmd('Network.setUserAgentOverride', {\n",
    "        \"userAgent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'\n",
    "    })\n",
    "    \n",
    "    # Remove webdriver flags\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0362d18b-f56f-4482-af31-d6dd8c5e4565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING MULTI-DATE ARCHIVE COLLECTION\n",
      "============================================================\n",
      "Date Range: 2025-06-05 to 2025-07-05\n",
      "Total Dates: 31\n",
      "Estimated Time: ~93 minutes\n",
      "============================================================\n",
      "\n",
      "############################################################\n",
      "DATE 1/31: 2025-06-05\n",
      "############################################################\n",
      "\n",
      "üåê Launching browser for this date...\n",
      "\n",
      "============================================================\n",
      "üìÖ SCRAPING DATE: 2025-06-05\n",
      "============================================================\n",
      "  üìÑ Page 1/12...     Page title: Just a moment...\n",
      "‚ùå Failed - Message: \n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x8158d3\n",
      "\t0x815914\n",
      "\t0x62e76d\n",
      "\t0x67a09d\n",
      "\t0x67a47b\n",
      "\t0x6c1802\n",
      "\t0x69c9b4\n",
      "\t0x6beea6\n",
      "\t0x69c766\n",
      "\t0x66dac0\n",
      "\t0x66ede4\n",
      "\t0xa97974\n",
      "\t0xa92bea\n",
      "\t0x83e5c4\n",
      "\t0x82dd38\n",
      "\t0x834d9d\n",
      "\t0x81dee8\n",
      "\t0x81e0ac\n",
      "\t0x807d2a\n",
      "\t0x76d35d49\n",
      "\t0x77cad6db\n",
      "\t0x77cad661\n",
      "...\n",
      "  üìÑ Page 2/12... üîí Closing browser for this date...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m driver \u001b[38;5;241m=\u001b[39m initialize_selenium_driver()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Scrape this date\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m df_date \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_archive_for_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m date_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m date_start\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 37\u001b[0m, in \u001b[0;36mscrape_archive_for_date\u001b[1;34m(target_date, driver, total_pages)\u001b[0m\n\u001b[0;32m     34\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Wait longer for Cloudflare to resolve\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Increased wait time\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Check if we're past Cloudflare\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Page title: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdriver\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MAIN MULTI-DATE SCRAPING LOOP (STABLE VERSION)\n",
    "# ============================================\n",
    "print(\"üöÄ STARTING MULTI-DATE ARCHIVE COLLECTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"Total Dates: {len(date_list)}\")\n",
    "print(f\"Estimated Time: ~{len(date_list) * 3} minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize storage for all results\n",
    "all_dataframes = []\n",
    "scraping_stats = []\n",
    "\n",
    "overall_start = time.time()\n",
    "\n",
    "# Scrape each date with its own browser instance\n",
    "for i, date in enumerate(date_list, 1):\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"DATE {i}/{len(date_list)}: {date}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    date_start = time.time()\n",
    "    driver = None\n",
    "    \n",
    "    try:\n",
    "        # Initialize fresh browser for this date\n",
    "        print(\"\\nüåê Launching browser for this date...\")\n",
    "        driver = initialize_selenium_driver()\n",
    "        \n",
    "        # Scrape this date\n",
    "        df_date = scrape_archive_for_date(date, driver)\n",
    "        \n",
    "        date_elapsed = time.time() - date_start\n",
    "        \n",
    "        # Store results\n",
    "        all_dataframes.append(df_date)\n",
    "        scraping_stats.append({\n",
    "            'date': date,\n",
    "            'articles_scraped': len(df_date),\n",
    "            'time_seconds': date_elapsed\n",
    "        })\n",
    "        \n",
    "        print(f\"‚è±Ô∏è Time for this date: {date_elapsed:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {date}: {type(e).__name__}: {str(e)[:100]}\")\n",
    "        # Add empty dataframe to maintain alignment\n",
    "        all_dataframes.append(pd.DataFrame(columns=['title', 'category', 'link', 'archive_date', \n",
    "                                                     'source', 'scraped_at', 'has_summary', 'needs_full_scrape']))\n",
    "        scraping_stats.append({\n",
    "            'date': date,\n",
    "            'articles_scraped': 0,\n",
    "            'time_seconds': 0\n",
    "        })\n",
    "    \n",
    "    finally:\n",
    "        # Always close browser for this date\n",
    "        if driver:\n",
    "            print(\"üîí Closing browser for this date...\")\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Rate limiting between dates\n",
    "    if i < len(date_list):\n",
    "        print(f\"‚è≥ Waiting {RATE_LIMIT_BETWEEN_DATES}s before next date...\\n\")\n",
    "        time.sleep(RATE_LIMIT_BETWEEN_DATES)\n",
    "\n",
    "overall_elapsed = time.time() - overall_start\n",
    "\n",
    "# Combine all DataFrames\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîÑ COMBINING RESULTS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if all_dataframes:\n",
    "    df_all_dates = pd.concat(all_dataframes, ignore_index=True)\n",
    "else:\n",
    "    df_all_dates = pd.DataFrame()\n",
    "\n",
    "print(f\"‚úÖ Collection Complete!\")\n",
    "print(f\"‚è±Ô∏è Total Time: {overall_elapsed:.1f}s ({overall_elapsed/60:.1f} minutes)\")\n",
    "print(f\"üìä Total Articles Collected: {len(df_all_dates)}\")\n",
    "\n",
    "# Display per-date stats\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä PER-DATE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "for stat in scraping_stats:\n",
    "    print(f\"{stat['date']}: {stat['articles_scraped']} articles in {stat['time_seconds']:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05303968-720c-4ac6-8653-5c03fb74b3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Transforming data for upload...\n",
      "‚úÖ Transformed 1690 records\n",
      "\n",
      "üöÄ Starting upload to Supabase...\n",
      "============================================================\n",
      "üì§ Uploading batch 1/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 2/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 3/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 4/34 (50 records)... ‚ùå Error: {'message': 'ON CONFLICT DO UPDATE command cannot affect row a second time', 'code': '21000', 'hint': 'Ensure that no ro\n",
      "üì§ Uploading batch 5/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 6/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 7/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 8/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 9/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 10/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 11/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 12/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 13/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 14/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 15/34 (50 records)... ‚ùå Error: {'message': 'ON CONFLICT DO UPDATE command cannot affect row a second time', 'code': '21000', 'hint': 'Ensure that no ro\n",
      "üì§ Uploading batch 16/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 17/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 18/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 19/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 20/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 21/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 22/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 23/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 24/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 25/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 26/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 27/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 28/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 29/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 30/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 31/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 32/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 33/34 (50 records)... ‚úÖ Done\n",
      "üì§ Uploading batch 34/34 (40 records)... ‚úÖ Done\n",
      "============================================================\n",
      "‚úÖ Upload Complete!\n",
      "   Uploaded or Updated: 1590\n",
      "   Errors: 100\n",
      "\n",
      "üìä Total articles in database: 2744\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# UPLOAD TO SUPABASE (Enhanced & Safe Version)\n",
    "# ============================================\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def transform_archive_for_upload(df_archive):\n",
    "    \"\"\"\n",
    "    Transform Archive DataFrame to match Supabase schema\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for _, row in df_archive.iterrows():\n",
    "        def safe_str(value):\n",
    "            if pd.isna(value) or value == '' or value == 'None':\n",
    "                return None\n",
    "            return str(value).strip()\n",
    "        \n",
    "        record = {\n",
    "            'title': safe_str(row.get('title')),\n",
    "            'link': safe_str(row.get('link')),\n",
    "            'category': safe_str(row.get('category')),\n",
    "            'source': 'The Hindu - Archive',\n",
    "            'pub_date': None,\n",
    "            'archive_date': safe_str(row.get('archive_date')),\n",
    "            'description': None,\n",
    "            'content_full': None,\n",
    "            'guid': None,\n",
    "            'image_url': None,\n",
    "            'image_width': None,\n",
    "            'image_height': None,\n",
    "            'has_description': False,\n",
    "            'has_image': False,\n",
    "            'needs_full_scrape': True,\n",
    "            'raw_json': None,\n",
    "            'scraped_at': safe_str(row.get('scraped_at'))\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "\n",
    "def upload_records_batch(records, batch_size=50):\n",
    "    \"\"\"\n",
    "    Upload or update records in batches using UPSERT (no duplicates)\n",
    "    \"\"\"\n",
    "    total = len(records)\n",
    "    uploaded = 0\n",
    "    updated = 0\n",
    "    errors = 0\n",
    "\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = records[i:i+batch_size]\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        total_batches = (total // batch_size) + 1\n",
    "\n",
    "        print(f\"üì§ Uploading batch {batch_num}/{total_batches} ({len(batch)} records)...\", end=\" \")\n",
    "\n",
    "        try:\n",
    "            # ‚úÖ Use UPSERT ‚Äî avoids duplicate key errors\n",
    "            result = supabase.table('news_cleaned').upsert(batch, on_conflict='link').execute()\n",
    "\n",
    "            # Some Supabase clients don't directly show affected rows; you can infer:\n",
    "            uploaded += len(batch)\n",
    "\n",
    "            print(f\"‚úÖ Done\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"‚ùå Error: {error_msg[:120]}\")\n",
    "            errors += len(batch)\n",
    "\n",
    "    return uploaded, updated, errors\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# RUN UPLOAD PROCESS\n",
    "# ============================================\n",
    "\n",
    "print(\"üîÑ Transforming data for upload...\")\n",
    "archive_records = transform_archive_for_upload(df_all_dates)\n",
    "print(f\"‚úÖ Transformed {len(archive_records)} records\")\n",
    "\n",
    "print(\"\\nüöÄ Starting upload to Supabase...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "uploaded, updated, errors = upload_records_batch(archive_records, batch_size=50)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Upload Complete!\")\n",
    "print(f\"   Uploaded or Updated: {uploaded}\")\n",
    "print(f\"   Errors: {errors}\")\n",
    "\n",
    "# Verify total count in database\n",
    "try:\n",
    "    result = supabase.table('news_cleaned').select(\"*\", count='exact').execute()\n",
    "    print(f\"\\nüìä Total articles in database: {result.count}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not fetch count: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66106818-5955-4ada-a93b-19f80f6b9dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
