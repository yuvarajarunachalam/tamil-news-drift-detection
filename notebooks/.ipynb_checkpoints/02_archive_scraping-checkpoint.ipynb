{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "869220be-a04d-43bf-897a-f10ec1acb680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f13fe439-9ea9-48eb-8cbc-4f1969bf6ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Target Date: 2025-09-27\n",
      "üìÑ Pages to Scrape: 12\n",
      "üè∑Ô∏è  TN Categories Configured: 26\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_ARCHIVE_URL = \"https://www.thehindu.com/archive/web/2025/09/27/\"\n",
    "TARGET_DATE = \"2025-09-27\"\n",
    "TOTAL_PAGES = 12  # Based on your manual check\n",
    "\n",
    "# Tamil Nadu categories to filter\n",
    "TN_CATEGORIES = {\n",
    "    'tamil nadu',\n",
    "    'chennai', 'coimbatore', 'madurai',\n",
    "    'tiruchirappalli', 'tiruchi', 'trichy',\n",
    "    'salem', 'tirunelveli', 'erode', 'vellore',\n",
    "    'thoothukudi', 'tuticorin', 'tiruppur',\n",
    "    'dindigul', 'thanjavur', 'nagercoil',\n",
    "    'kanyakumari', 'kanniyakumari', 'karur',\n",
    "    'pudukkottai', 'cuddalore', 'hosur',\n",
    "    'kanchipuram', 'avadi', 'kancheepuram'\n",
    "}\n",
    "\n",
    "# Scraping settings\n",
    "RATE_LIMIT_SECONDS = 2\n",
    "REQUEST_TIMEOUT = 10\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "\n",
    "print(f\"üéØ Target Date: {TARGET_DATE}\")\n",
    "print(f\"üìÑ Pages to Scrape: {TOTAL_PAGES}\")\n",
    "print(f\"üè∑Ô∏è  TN Categories Configured: {len(TN_CATEGORIES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "949ca388-f090-4614-95d8-cab49dc7ceab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing: Fetching Page 1 with Selenium (real browser)...\n",
      "\n",
      "‚úÖ Page fetched successfully!\n",
      "üìè HTML Size: 201691 characters\n"
     ]
    }
   ],
   "source": [
    "# Install selenium and webdriver\n",
    "!pip install selenium webdriver-manager\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def fetch_archive_page_selenium(page_num=1):\n",
    "    \"\"\"\n",
    "    Fetch archive page using Selenium (real browser)\n",
    "    \"\"\"\n",
    "    if page_num == 1:\n",
    "        url = BASE_ARCHIVE_URL\n",
    "    else:\n",
    "        url = f\"{BASE_ARCHIVE_URL}?page={page_num}\"\n",
    "    \n",
    "    # Setup Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')  # Run without opening browser window\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "    \n",
    "    try:\n",
    "        # Initialize driver\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        \n",
    "        # Load page\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for content to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"element\"))\n",
    "        )\n",
    "        \n",
    "        # Get page source\n",
    "        html_content = driver.page_source\n",
    "        \n",
    "        # Close browser\n",
    "        driver.quit()\n",
    "        \n",
    "        return html_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Selenium error on page {page_num}: {e}\")\n",
    "        if 'driver' in locals():\n",
    "            driver.quit()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test Selenium approach\n",
    "print(\"üîç Testing: Fetching Page 1 with Selenium (real browser)...\\n\")\n",
    "html_content = fetch_archive_page_selenium(1)\n",
    "\n",
    "if html_content:\n",
    "    print(f\"‚úÖ Page fetched successfully!\")\n",
    "    print(f\"üìè HTML Size: {len(html_content)} characters\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to fetch page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0afee8f1-a515-484f-a274-6b7a5f0bcc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing: Parsing articles from Page 1...\n",
      "\n",
      "‚úÖ Found 40 articles on Page 1\n",
      "\n",
      "First 3 articles:\n",
      "\n",
      "1. Category: World\n",
      "   Title: Moldova bans another pro-Russian party from September 28's vote...\n",
      "   Link: https://www.thehindu.com/news/international/moldova-bans-another-pro-russian-party-from-september-28s-vote/article70103200.ece\n",
      "\n",
      "2. Category: Kerala\n",
      "   Title: Bison caught in residential area released into Aralam sanctuary...\n",
      "   Link: https://www.thehindu.com/news/national/kerala/bison-caught-in-residential-area-released-into-aralam-sanctuary/article70102936.ece\n",
      "\n",
      "3. Category: World\n",
      "   Title: Russia says seized three villages in east Ukraine...\n",
      "   Link: https://www.thehindu.com/news/international/russia-says-seized-three-villages-in-east-ukraine/article70103203.ece\n"
     ]
    }
   ],
   "source": [
    "def parse_articles_from_html(html_content):\n",
    "    \"\"\"\n",
    "    Parse all articles from archive page HTML\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all article elements\n",
    "    article_elements = soup.find_all('div', class_='element')\n",
    "    \n",
    "    articles = []\n",
    "    \n",
    "    for element in article_elements:\n",
    "        # Extract category label\n",
    "        label_div = element.find('div', class_='label')\n",
    "        category = label_div.get_text(strip=True) if label_div else ''\n",
    "        \n",
    "        # Extract title and link\n",
    "        title_div = element.find('div', class_='title')\n",
    "        if title_div:\n",
    "            link_tag = title_div.find('a')\n",
    "            if link_tag:\n",
    "                title = link_tag.get_text(strip=True)\n",
    "                link = link_tag.get('href', '')\n",
    "                \n",
    "                articles.append({\n",
    "                    'category': category,\n",
    "                    'title': title,\n",
    "                    'link': link\n",
    "                })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "\n",
    "# Test: Parse first page\n",
    "print(\"üîç Testing: Parsing articles from Page 1...\\n\")\n",
    "articles_page1 = parse_articles_from_html(html_content)\n",
    "\n",
    "print(f\"‚úÖ Found {len(articles_page1)} articles on Page 1\")\n",
    "print(\"\\nFirst 3 articles:\")\n",
    "for i, article in enumerate(articles_page1[:3], 1):\n",
    "    print(f\"\\n{i}. Category: {article['category']}\")\n",
    "    print(f\"   Title: {article['title'][:80]}...\")\n",
    "    print(f\"   Link: {article['link']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55008c0c-7245-4ffd-a835-89209717570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä FILTERING RESULTS - PAGE 1\n",
      "============================================================\n",
      "Total Articles: 40\n",
      "Tamil Nadu Articles: 7\n",
      "Filter Rate: 17.5%\n",
      "\n",
      "============================================================\n",
      "üéØ TAMIL NADU ARTICLES FOUND:\n",
      "============================================================\n",
      "\n",
      "1. [Tamil Nadu] Encroachments demolished to complete new bridge across Kamadalam river near Arani\n",
      "   https://www.thehindu.com/news/national/tamil-nadu/encroachments-demolished-to-complete-new-bridge-across-kamadalam-river-near-arani/article70102100.ece\n",
      "\n",
      "2. [Tamil Nadu] Engineering students exhibit futuristic skills at graVITas‚Äô25\n",
      "   https://www.thehindu.com/news/national/tamil-nadu/engineering-students-exhibit-futuristic-skills-at-gravitas25/article70102115.ece\n",
      "\n",
      "3. [Chennai] Pheasant-tailed jacana: raising more¬†families\n",
      "   https://www.thehindu.com/news/cities/chennai/pheasant-tailed-jacana-raising-more-families/article70103084.ece\n",
      "\n",
      "4. [Coimbatore] Suicide pact: third sibling succumbs in Coimbatore hospital\n",
      "   https://www.thehindu.com/news/cities/Coimbatore/suicide-pact-third-sibling-succumbs-in-coimbatore-hospital/article70103014.ece\n",
      "\n",
      "5. [Chennai] Art challenge driven by AI celebrates Durga Puja\n",
      "   https://www.thehindu.com/news/cities/chennai/art-challenge-driven-by-ai-celebrates-durga-puja/article70103067.ece\n",
      "\n",
      "6. [Chennai] A major operation to¬†rescue blind dog\n",
      "   https://www.thehindu.com/news/cities/chennai/a-major-operation-to-rescue-blind-dog/article70103055.ece\n",
      "\n",
      "7. [Madurai] Co-optex festival special sale attracts customers in Ramanathapuram\n",
      "   https://www.thehindu.com/news/cities/Madurai/co-optex-festival-special-sale-attracts-customers-in-ramanathapuram/article70102967.ece\n"
     ]
    }
   ],
   "source": [
    "def is_tamil_nadu_article(category):\n",
    "    \"\"\"\n",
    "    Check if article belongs to Tamil Nadu\n",
    "    \"\"\"\n",
    "    return category.lower() in TN_CATEGORIES\n",
    "\n",
    "\n",
    "# Test: Filter Page 1 articles\n",
    "tn_articles_page1 = [a for a in articles_page1 if is_tamil_nadu_article(a['category'])]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä FILTERING RESULTS - PAGE 1\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Articles: {len(articles_page1)}\")\n",
    "print(f\"Tamil Nadu Articles: {len(tn_articles_page1)}\")\n",
    "print(f\"Filter Rate: {len(tn_articles_page1)/len(articles_page1)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ TAMIL NADU ARTICLES FOUND:\")\n",
    "print(\"=\" * 60)\n",
    "for i, article in enumerate(tn_articles_page1, 1):\n",
    "    print(f\"\\n{i}. [{article['category']}] {article['title']}\")\n",
    "    print(f\"   {article['link']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afca3c9f-b340-4272-b535-8eec4859b7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting full scrape of all 12 pages with Selenium...\n",
      "============================================================\n",
      "üöÄ Initializing Chrome browser...\n",
      "\n",
      "üìÑ Scraping Page 1/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 2/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 3/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 4/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 5/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 6/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 7/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 8/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 9/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 10/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 11/12... ‚úÖ Found 40 articles\n",
      "üìÑ Scraping Page 12/12... ‚úÖ Found 21 articles\n",
      "\n",
      "üîí Closing browser...\n",
      "============================================================\n",
      "‚úÖ Scraping Complete!\n",
      "‚è±Ô∏è  Time Taken: 56.3 seconds (0.9 minutes)\n",
      "üìä Total Articles Collected: 461\n"
     ]
    }
   ],
   "source": [
    "def scrape_all_pages_selenium(total_pages):\n",
    "    \"\"\"\n",
    "    Scrape all pages for the target date using Selenium\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    \n",
    "    # Setup Chrome options ONCE (reuse for all pages)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "    \n",
    "    # Initialize driver ONCE\n",
    "    print(\"üöÄ Initializing Chrome browser...\\n\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        for page_num in range(1, total_pages + 1):\n",
    "            print(f\"üìÑ Scraping Page {page_num}/{total_pages}...\", end=\" \")\n",
    "            \n",
    "            # Construct URL\n",
    "            if page_num == 1:\n",
    "                url = BASE_ARCHIVE_URL\n",
    "            else:\n",
    "                url = f\"{BASE_ARCHIVE_URL}?page={page_num}\"\n",
    "            \n",
    "            try:\n",
    "                # Load page\n",
    "                driver.get(url)\n",
    "                \n",
    "                # Wait for articles to load\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"element\"))\n",
    "                )\n",
    "                \n",
    "                # Small additional delay to ensure all content loaded\n",
    "                time.sleep(1)\n",
    "                \n",
    "                # Get page source\n",
    "                html_content = driver.page_source\n",
    "                \n",
    "                # Parse articles\n",
    "                articles = parse_articles_from_html(html_content)\n",
    "                all_articles.extend(articles)\n",
    "                \n",
    "                print(f\"‚úÖ Found {len(articles)} articles\")\n",
    "                \n",
    "                # Rate limiting between pages\n",
    "                if page_num < total_pages:\n",
    "                    time.sleep(RATE_LIMIT_SECONDS)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed - {str(e)[:50]}... - Skipping\")\n",
    "                continue\n",
    "        \n",
    "    finally:\n",
    "        # Always close browser when done\n",
    "        print(\"\\nüîí Closing browser...\")\n",
    "        driver.quit()\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "\n",
    "# Execute: Scrape all pages\n",
    "print(\"üöÄ Starting full scrape of all 12 pages with Selenium...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "all_articles = scrape_all_pages_selenium(TOTAL_PAGES)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Scraping Complete!\")\n",
    "print(f\"‚è±Ô∏è  Time Taken: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "print(f\"üìä Total Articles Collected: {len(all_articles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99a6310e-52e1-4584-83b3-690e2a423335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä COMPLETE FILTERING RESULTS\n",
      "============================================================\n",
      "Total Articles Scraped: 461\n",
      "Tamil Nadu Articles: 61\n",
      "Filter Rate: 13.2%\n",
      "Articles Excluded: 400\n",
      "\n",
      "============================================================\n",
      "üè∑Ô∏è  CATEGORY BREAKDOWN (Tamil Nadu Only)\n",
      "============================================================\n",
      "Tamil Nadu: 19\n",
      "Madurai: 15\n",
      "Chennai: 14\n",
      "Coimbatore: 13\n"
     ]
    }
   ],
   "source": [
    "# Filter for Tamil Nadu articles\n",
    "tn_articles_all = [a for a in all_articles if is_tamil_nadu_article(a['category'])]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä COMPLETE FILTERING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Articles Scraped: {len(all_articles)}\")\n",
    "print(f\"Tamil Nadu Articles: {len(tn_articles_all)}\")\n",
    "print(f\"Filter Rate: {len(tn_articles_all)/len(all_articles)*100:.1f}%\")\n",
    "print(f\"Articles Excluded: {len(all_articles) - len(tn_articles_all)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üè∑Ô∏è  CATEGORY BREAKDOWN (Tamil Nadu Only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count by category\n",
    "category_counts = {}\n",
    "for article in tn_articles_all:\n",
    "    cat = article['category']\n",
    "    category_counts[cat] = category_counts.get(cat, 0) + 1\n",
    "\n",
    "for cat, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{cat}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67f8ffee-c209-4032-a0ef-a2f0834c02ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä DATAFRAME CREATED\n",
      "============================================================\n",
      "Shape: (61, 8)\n",
      "\n",
      "Columns: ['title', 'category', 'link', 'archive_date', 'has_summary', 'needs_full_scrape', 'source', 'scraped_at']\n",
      "\n",
      "============================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61 entries, 0 to 60\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   title              61 non-null     object\n",
      " 1   category           61 non-null     object\n",
      " 2   link               61 non-null     object\n",
      " 3   archive_date       61 non-null     object\n",
      " 4   has_summary        61 non-null     bool  \n",
      " 5   needs_full_scrape  61 non-null     bool  \n",
      " 6   source             61 non-null     object\n",
      " 7   scraped_at         61 non-null     object\n",
      "dtypes: bool(2), object(6)\n",
      "memory usage: 3.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Add metadata to each article\n",
    "for article in tn_articles_all:\n",
    "    article['archive_date'] = TARGET_DATE\n",
    "    article['source'] = 'The Hindu - Archive'\n",
    "    article['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    article['has_summary'] = False  # Archive pages don't have summaries\n",
    "    article['needs_full_scrape'] = True  # Will need to scrape article page\n",
    "\n",
    "# Create DataFrame\n",
    "df_archive = pd.DataFrame(tn_articles_all)\n",
    "\n",
    "# Reorder columns\n",
    "column_order = ['title', 'category', 'link', 'archive_date', 'has_summary', \n",
    "                'needs_full_scrape', 'source', 'scraped_at']\n",
    "df_archive = df_archive[column_order]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä DATAFRAME CREATED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df_archive.shape}\")\n",
    "print(f\"\\nColumns: {list(df_archive.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "df_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87662edc-50dd-4f6f-b351-b70fca0fe9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç DATA QUALITY CHECKS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Duplicate Links: 2\n",
      "‚úÖ Missing Titles: 0\n",
      "‚úÖ Missing Links: 0\n",
      "‚úÖ Missing Categories: 0\n",
      "\n",
      "üìè Title Length Stats:\n",
      "   Average: 67 characters\n",
      "   Min: 20\n",
      "   Max: 112\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üîç DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for duplicate links\n",
    "duplicate_links = df_archive['link'].duplicated().sum()\n",
    "print(f\"\\n‚úÖ Duplicate Links: {duplicate_links}\")\n",
    "\n",
    "# Check for missing fields\n",
    "print(f\"‚úÖ Missing Titles: {df_archive['title'].isna().sum()}\")\n",
    "print(f\"‚úÖ Missing Links: {df_archive['link'].isna().sum()}\")\n",
    "print(f\"‚úÖ Missing Categories: {df_archive['category'].isna().sum()}\")\n",
    "\n",
    "# Title length stats\n",
    "df_archive['title_length'] = df_archive['title'].str.len()\n",
    "print(f\"\\nüìè Title Length Stats:\")\n",
    "print(f\"   Average: {df_archive['title_length'].mean():.0f} characters\")\n",
    "print(f\"   Min: {df_archive['title_length'].min()}\")\n",
    "print(f\"   Max: {df_archive['title_length'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45542a63-cf98-45f5-9000-c2e44c6457e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data exported to: C:\\Users\\Yuvaraj\\Desktop\\Data-Science\\Mini-project\\tamil-news-drift-detection/data/raw/archive_sept27_tn_20251027_132501.csv\n",
      "üìä Exported 61 Tamil Nadu articles\n"
     ]
    }
   ],
   "source": [
    "# Uncomment when ready to export\n",
    "\n",
    "output_filename = rf\"C:\\Users\\Yuvaraj\\Desktop\\Data-Science\\Mini-project\\tamil-news-drift-detection/data/raw/archive_sept27_tn_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_archive.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"‚úÖ Data exported to: {output_filename}\")\n",
    "print(f\"üìä Exported {len(df_archive)} Tamil Nadu articles\")\n",
    "\n",
    "# print(\"üí° Uncomment the code above when ready to export to CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172925f0-7776-4e75-a7f6-9217f23da9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
