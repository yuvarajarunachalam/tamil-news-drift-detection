{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664633b8-0a5f-41bb-82cf-04e77effc397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment loaded successfully\n",
      "ðŸ“Š Supabase URL: https://lgnhjzlbezpczlobeevu.s...\n",
      "ðŸ”‘ API Key loaded: 208 characters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize Supabase client\n",
    "SUPABASE_URL = \"https://lgnhjzlbezpczlobeevu.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImxnbmhqemxiZXpwY3psb2JlZXZ1Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTgyMDgzNjcsImV4cCI6MjA3Mzc4NDM2N30.O5Yt0dOyYq326ESo0LBL7lGj4k8zwpuodJfTtGwrPek\"  # Replace with your actual key\n",
    "\n",
    "if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "    raise ValueError(\"Missing Supabase credentials\")\n",
    "\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "print(\"âœ… Environment loaded successfully\")\n",
    "print(f\"ðŸ“Š Supabase URL: {SUPABASE_URL[:30]}...\")\n",
    "print(f\"ðŸ”‘ API Key loaded: {len(SUPABASE_KEY)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10271328-d6a6-4d8a-b1b8-5931a7c346bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“° Found 68 RSS articles needing full content\n",
      "\n",
      "ðŸ“‹ Sample articles:\n",
      "                                     id  \\\n",
      "0  41eb8377-f89f-499a-8f3a-212e537b42de   \n",
      "1  6067d8d8-dff9-47c8-8121-b6ea7fa6f619   \n",
      "2  7c9518c5-2224-4cd6-afc5-4318c78237c7   \n",
      "3  a29498b2-9a64-4e13-b78b-7edd00272200   \n",
      "4  c468b884-94b3-4973-9f05-6601255010c7   \n",
      "\n",
      "                                               title  \n",
      "0  Orange alert issued in north Tamil Nadu distri...  \n",
      "1  Kalaignar International Convention Centre set ...  \n",
      "2  Eminent plastic surgeon K. Mathangi Ramakrishn...  \n",
      "3  What is the problem faced by paddy farmers of ...  \n",
      "4                  BM reviews works in Adyar estuary  \n"
     ]
    }
   ],
   "source": [
    "# Fetch all RSS articles where content_full is NULL or empty\n",
    "response = supabase.table('news_cleaned')\\\n",
    "    .select('id, title, link, source')\\\n",
    "    .eq('source', 'The Hindu - RSS')\\\n",
    "    .is_('content_full', 'null')\\\n",
    "    .execute()\n",
    "\n",
    "rss_articles = pd.DataFrame(response.data)\n",
    "\n",
    "print(f\"ðŸ“° Found {len(rss_articles)} RSS articles needing full content\")\n",
    "print(f\"\\nðŸ“‹ Sample articles:\")\n",
    "print(rss_articles[['id', 'title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7169687d-6a53-4508-aa26-eab1e86c266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Test Scrape:\n",
      "URL: https://www.thehindu.com/news/national/tamil-nadu/cyclone-montha-orange-alert-issued-in-north-tamil-nadu-districts-as-weather-system-advances/article70207325.ece\n",
      "âœ… Success! Content length: 3440 characters\n",
      "Preview: Cyclone Monthais expected to bring heavy rainfall over some parts of north Tamil Nadu until it crosses the coast on Tuesday (October 28, 2025) evening or night. The Regional Meteorological Centre (RMC...\n"
     ]
    }
   ],
   "source": [
    "def scrape_hindu_article(url, max_retries=3):\n",
    "    \"\"\"\n",
    "    Scrape full article content from The Hindu\n",
    "    Returns: (content_text, error_message)\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # The Hindu article content is typically in <div> with class 'articlebodycontent'\n",
    "            # Try multiple selectors\n",
    "            content = None\n",
    "            \n",
    "            # Selector 1: Main article body\n",
    "            article_body = soup.find('div', class_='articlebodycontent')\n",
    "            if article_body:\n",
    "                paragraphs = article_body.find_all('p')\n",
    "                content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "            \n",
    "            # Selector 2: Alternative structure\n",
    "            if not content:\n",
    "                article_body = soup.find('div', {'itemprop': 'articleBody'})\n",
    "                if article_body:\n",
    "                    paragraphs = article_body.find_all('p')\n",
    "                    content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "            \n",
    "            # Selector 3: Generic article tag\n",
    "            if not content:\n",
    "                article = soup.find('article')\n",
    "                if article:\n",
    "                    paragraphs = article.find_all('p')\n",
    "                    content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "            \n",
    "            if content and len(content) > 100:  # Ensure we got meaningful content\n",
    "                # Clean the content\n",
    "                content = re.sub(r'\\s+', ' ', content).strip()\n",
    "                return content, None\n",
    "            else:\n",
    "                return None, \"Content too short or not found\"\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return None, \"Timeout after retries\"\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return None, f\"Request error: {str(e)}\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            return None, f\"Parsing error: {str(e)}\"\n",
    "    \n",
    "    return None, \"Max retries exceeded\"\n",
    "\n",
    "\n",
    "# Test the function on first article\n",
    "test_url = rss_articles.iloc[0]['link']\n",
    "test_content, test_error = scrape_hindu_article(test_url)\n",
    "\n",
    "print(\"ðŸ§ª Test Scrape:\")\n",
    "print(f\"URL: {test_url}\")\n",
    "if test_content:\n",
    "    print(f\"âœ… Success! Content length: {len(test_content)} characters\")\n",
    "    print(f\"Preview: {test_content[:200]}...\")\n",
    "else:\n",
    "    print(f\"âŒ Failed: {test_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31fcf80c-21eb-44fd-bd5d-d85a4a4c771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting scrape of 68 articles...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250a194ec9454e12a6113bd009db5588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“Š Scraping Complete!\n",
      "âœ… Successful: 68\n",
      "âŒ Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# Storage for results\n",
    "results = {\n",
    "    'successful': [],\n",
    "    'failed': [],\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "print(f\"ðŸš€ Starting scrape of {len(rss_articles)} articles...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process each article with progress bar\n",
    "for idx, row in tqdm(rss_articles.iterrows(), total=len(rss_articles), desc=\"Scraping\"):\n",
    "    article_id = row['id']\n",
    "    link = row['link']\n",
    "    title = row['title']\n",
    "    \n",
    "    # Scrape content\n",
    "    content, error = scrape_hindu_article(link)\n",
    "    \n",
    "    if content:\n",
    "        results['successful'].append({\n",
    "            'id': article_id,\n",
    "            'title': title,\n",
    "            'content': content,\n",
    "            'content_length': len(content)\n",
    "        })\n",
    "    else:\n",
    "        results['failed'].append({\n",
    "            'id': article_id,\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'error': error\n",
    "        })\n",
    "        results['errors'].append(error)\n",
    "    \n",
    "    # Rate limiting - be polite to The Hindu's servers\n",
    "    time.sleep(1)  # 1 second between requests\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š Scraping Complete!\")\n",
    "print(f\"âœ… Successful: {len(results['successful'])}\")\n",
    "print(f\"âŒ Failed: {len(results['failed'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696c86a3-9d80-4a6d-96ce-815a144281e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Updating Supabase with scraped content...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cd498f0bd94baeb918eba239b652ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating DB:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… Database Update Complete!\n",
      "   - Successfully updated: 68\n",
      "   - Failed to update: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ’¾ Updating Supabase with scraped content...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "update_success = 0\n",
    "update_failed = 0\n",
    "\n",
    "for article in tqdm(results['successful'], desc=\"Updating DB\"):\n",
    "    try:\n",
    "        response = supabase.table('news_cleaned')\\\n",
    "            .update({\n",
    "                'content_full': article['content'],\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            })\\\n",
    "            .eq('id', article['id'])\\\n",
    "            .execute()\n",
    "        \n",
    "        update_success += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Failed to update {article['id']}: {e}\")\n",
    "        update_failed += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Database Update Complete!\")\n",
    "print(f\"   - Successfully updated: {update_success}\")\n",
    "print(f\"   - Failed to update: {update_failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd16be6-8a3d-4957-8393-01d505826195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“‹ FINAL SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "ðŸ“° Total RSS Articles Processed: 68\n",
      "âœ… Successfully Scraped: 68 (100.0%)\n",
      "âŒ Failed to Scrape: 0 (0.0%)\n",
      "ðŸ’¾ Database Updates: 68 successful, 0 failed\n",
      "\n",
      "ðŸ“ Content Length Statistics:\n",
      "   - Min: 130 characters\n",
      "   - Max: 29787 characters\n",
      "   - Average: 2870 characters\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ‰ Scraping pipeline complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“‹ FINAL SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“° Total RSS Articles Processed: {len(rss_articles)}\")\n",
    "print(f\"âœ… Successfully Scraped: {len(results['successful'])} ({len(results['successful'])/len(rss_articles)*100:.1f}%)\")\n",
    "print(f\"âŒ Failed to Scrape: {len(results['failed'])} ({len(results['failed'])/len(rss_articles)*100:.1f}%)\")\n",
    "print(f\"ðŸ’¾ Database Updates: {update_success} successful, {update_failed} failed\")\n",
    "\n",
    "# Content length statistics\n",
    "if results['successful']:\n",
    "    content_lengths = [r['content_length'] for r in results['successful']]\n",
    "    print(f\"\\nðŸ“ Content Length Statistics:\")\n",
    "    print(f\"   - Min: {min(content_lengths)} characters\")\n",
    "    print(f\"   - Max: {max(content_lengths)} characters\")\n",
    "    print(f\"   - Average: {sum(content_lengths)/len(content_lengths):.0f} characters\")\n",
    "\n",
    "# Show failed articles\n",
    "if results['failed']:\n",
    "    print(f\"\\nâŒ Failed Articles ({len(results['failed'])}):\")\n",
    "    failed_df = pd.DataFrame(results['failed'])\n",
    "    print(failed_df[['title', 'error']].to_string(index=False))\n",
    "    \n",
    "    # Error breakdown\n",
    "    print(f\"\\nðŸ” Error Breakdown:\")\n",
    "    error_counts = pd.Series(results['errors']).value_counts()\n",
    "    print(error_counts)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ‰ Scraping pipeline complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aba510-95b7-4631-b259-b195ddc787ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
