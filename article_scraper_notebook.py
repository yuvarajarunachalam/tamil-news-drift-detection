{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# 04: Full-Text Article Scraper\n",
    "**Goal**: Extract full article content for records missing descriptions\n",
    "\n",
    "**Strategy**:\n",
    "1. Test trafilatura on sample articles\n",
    "2. Fetch articles needing scraping from Supabase\n",
    "3. Extract content in batches with error handling\n",
    "4. Update database with extracted content\n",
    "5. Generate quality report\n",
    "\n",
    "**Target**: 82 articles (50 archive + 32 RSS without descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1-header",
   "metadata": {},
   "source": [
    "## Phase 1: Setup & Library Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run once)\n",
    "# !pip install trafilatura supabase python-dotenv requests\n",
    "\n",
    "import trafilatura\n",
    "from supabase import create_client, Client\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"Trafilatura version: {trafilatura.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supabase-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SUPABASE CREDENTIALS - REPLACE THESE\n",
    "# ============================================\n",
    "SUPABASE_URL = \"your_supabase_url_here\"\n",
    "SUPABASE_KEY = \"your_supabase_key_here\"\n",
    "# ============================================\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "print(\"‚úÖ Supabase client initialized!\")\n",
    "print(f\"Connected to: {SUPABASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-single-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trafilatura on a single article\n",
    "TEST_URL = \"https://www.thehindu.com/news/national/tamil-nadu/encroachments-demolished-to-complete-new-bridge-across-kamadalam-river-near-arani/article70102100.ece\"\n",
    "\n",
    "print(f\"üß™ Testing extraction on: {TEST_URL}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Download HTML\n",
    "    downloaded = trafilatura.fetch_url(TEST_URL)\n",
    "    \n",
    "    if downloaded:\n",
    "        # Extract content\n",
    "        content = trafilatura.extract(downloaded, include_comments=False, include_tables=False)\n",
    "        \n",
    "        if content:\n",
    "            print(\"‚úÖ Extraction Successful!\\n\")\n",
    "            print(f\"Content Length: {len(content)} characters\")\n",
    "            print(f\"Word Count: ~{len(content.split())} words\\n\")\n",
    "            print(\"First 500 characters:\")\n",
    "            print(\"-\"*60)\n",
    "            print(content[:500])\n",
    "            print(\"-\"*60)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Extraction returned empty content\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to download page\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\nüí° If extraction worked, proceed. If failed, we'll need a custom scraper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2-header",
   "metadata": {},
   "source": [
    "## Phase 2: Fetch Target Articles from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fetch-articles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all articles needing full-text scraping\n",
    "print(\"üì• Fetching articles that need scraping...\\n\")\n",
    "\n",
    "result = supabase.table('news_raw') \\\n",
    "    .select('id, title, link, source, category') \\\n",
    "    .eq('needs_full_scrape', True) \\\n",
    "    .execute()\n",
    "\n",
    "articles_to_scrape = result.data\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä ARTICLES NEEDING SCRAPING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total: {len(articles_to_scrape)} articles\\n\")\n",
    "\n",
    "# Breakdown by source\n",
    "df_scrape = pd.DataFrame(articles_to_scrape)\n",
    "source_counts = df_scrape['source'].value_counts()\n",
    "\n",
    "print(\"By Source:\")\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"  {source}: {count}\")\n",
    "\n",
    "# Category breakdown\n",
    "print(\"\\nBy Category:\")\n",
    "category_counts = df_scrape['category'].value_counts()\n",
    "for cat, count in category_counts.head(5).items():\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "print(\"\\nüí° Recommendation: Start with 20 articles (10 archive + 10 RSS) for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-test-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select test batch: 10 archive + 10 RSS\n",
    "archive_articles = [a for a in articles_to_scrape if 'Archive' in a['source']][:10]\n",
    "rss_articles = [a for a in articles_to_scrape if 'RSS' in a['source']][:10]\n",
    "\n",
    "test_batch = archive_articles + rss_articles\n",
    "\n",
    "print(\"üéØ TEST BATCH SELECTED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Archive articles: {len(archive_articles)}\")\n",
    "print(f\"RSS articles: {len(rss_articles)}\")\n",
    "print(f\"Total test batch: {len(test_batch)}\\n\")\n",
    "\n",
    "print(\"Sample articles:\")\n",
    "for i, article in enumerate(test_batch[:3], 1):\n",
    "    print(f\"\\n{i}. [{article['source']}]\")\n",
    "    print(f\"   {article['title'][:60]}...\")\n",
    "    print(f\"   {article['link'][:70]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3-header",
   "metadata": {},
   "source": [
    "## Phase 3: Content Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraction-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_content(url: str, max_retries: int = 3, timeout: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract full article content from URL using trafilatura\n",
    "    \n",
    "    Returns:\n",
    "        Dict with keys: success, content, error, word_count\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'success': False,\n",
    "        'content': None,\n",
    "        'error': None,\n",
    "        'word_count': 0\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Download HTML\n",
    "            downloaded = trafilatura.fetch_url(url)\n",
    "            \n",
    "            if not downloaded:\n",
    "                result['error'] = 'Failed to download page'\n",
    "                continue\n",
    "            \n",
    "            # Extract content\n",
    "            content = trafilatura.extract(\n",
    "                downloaded,\n",
    "                include_comments=False,\n",
    "                include_tables=False,\n",
    "                no_fallback=False  # Try harder to extract content\n",
    "            )\n",
    "            \n",
    "            if content and len(content.strip()) > 100:  # Min 100 chars\n",
    "                result['success'] = True\n",
    "                result['content'] = content.strip()\n",
    "                result['word_count'] = len(content.split())\n",
    "                return result\n",
    "            else:\n",
    "                result['error'] = 'Empty or too short content'\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            result['error'] = f'Timeout (attempt {attempt + 1}/{max_retries})'\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            result['error'] = f'Network error: {str(e)[:50]}'\n",
    "            break  # Don't retry network errors\n",
    "        except Exception as e:\n",
    "            result['error'] = f'Extraction error: {str(e)[:50]}'\n",
    "            break\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            time.sleep(2)  # Wait before retry\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Extraction function defined\")\n",
    "print(\"Features: 3 retries, 10s timeout, min 100 chars validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-scraper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_batch(articles: List[Dict], rate_limit: float = 2.5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape multiple articles with progress tracking\n",
    "    \n",
    "    Args:\n",
    "        articles: List of dicts with 'id', 'link', 'title'\n",
    "        rate_limit: Seconds to wait between requests\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with scraping results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(articles)\n",
    "    \n",
    "    print(f\"üöÄ Starting batch scraping: {total} articles\")\n",
    "    print(f\"‚è±Ô∏è  Rate limit: {rate_limit}s between requests\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, article in enumerate(articles, 1):\n",
    "        print(f\"\\n[{i}/{total}] Scraping: {article['title'][:50]}...\")\n",
    "        print(f\"URL: {article['link'][:60]}...\")\n",
    "        \n",
    "        # Extract content\n",
    "        extraction = extract_article_content(article['link'])\n",
    "        \n",
    "        # Store result\n",
    "        result = {\n",
    "            'id': article['id'],\n",
    "            'title': article['title'],\n",
    "            'link': article['link'],\n",
    "            'source': article['source'],\n",
    "            'success': extraction['success'],\n",
    "            'content': extraction['content'],\n",
    "            'word_count': extraction['word_count'],\n",
    "            'error': extraction['error']\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if extraction['success']:\n",
    "            print(f\"‚úÖ Success! Extracted {extraction['word_count']} words\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed: {extraction['error']}\")\n",
    "        \n",
    "        # Rate limiting (skip on last article)\n",
    "        if i < total:\n",
    "            time.sleep(rate_limit)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Batch scraping complete!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Batch scraper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-test-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scraping on test batch\n",
    "print(f\"‚ö†Ô∏è  About to scrape {len(test_batch)} articles\")\n",
    "print(f\"Estimated time: ~{len(test_batch) * 2.5 / 60:.1f} minutes\\n\")\n",
    "\n",
    "# Start scraping\n",
    "scraping_results = scrape_articles_batch(test_batch, rate_limit=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze scraping results\n",
    "df_results = pd.DataFrame(scraping_results)\n",
    "\n",
    "success_count = df_results['success'].sum()\n",
    "failure_count = len(df_results) - success_count\n",
    "success_rate = (success_count / len(df_results)) * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä SCRAPING RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Attempted: {len(df_results)}\")\n",
    "print(f\"Successful: {success_count} ({success_rate:.1f}%)\")\n",
    "print(f\"Failed: {failure_count} ({100-success_rate:.1f}%)\\n\")\n",
    "\n",
    "# Success by source\n",
    "print(\"Success Rate by Source:\")\n",
    "source_success = df_results.groupby('source')['success'].agg(['sum', 'count'])\n",
    "source_success['rate'] = (source_success['sum'] / source_success['count'] * 100).round(1)\n",
    "for source, row in source_success.iterrows():\n",
    "    print(f\"  {source}: {row['sum']}/{row['count']} ({row['rate']}%)\")\n",
    "\n",
    "# Word count statistics (for successful extractions)\n",
    "successful_df = df_results[df_results['success'] == True]\n",
    "if len(successful_df) > 0:\n",
    "    print(f\"\\nWord Count Statistics:\")\n",
    "    print(f\"  Average: {successful_df['word_count'].mean():.0f} words\")\n",
    "    print(f\"  Median: {successful_df['word_count'].median():.0f} words\")\n",
    "    print(f\"  Min: {successful_df['word_count'].min()} words\")\n",
    "    print(f\"  Max: {successful_df['word_count'].max()} words\")\n",
    "\n",
    "# Error breakdown\n",
    "if failure_count > 0:\n",
    "    print(f\"\\nError Breakdown:\")\n",
    "    error_counts = df_results[df_results['success'] == False]['error'].value_counts()\n",
    "    for error, count in error_counts.items():\n",
    "        print(f\"  {error}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sample extractions\n",
    "print(\"=\"*60)\n",
    "print(\"üì∞ SAMPLE EXTRACTED CONTENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "successful_samples = df_results[df_results['success'] == True].head(3)\n",
    "\n",
    "for i, row in successful_samples.iterrows():\n",
    "    print(f\"\\n{i+1}. {row['title'][:60]}...\")\n",
    "    print(f\"   Source: {row['source']}\")\n",
    "    print(f\"   Words: {row['word_count']}\")\n",
    "    print(f\"   Content Preview:\")\n",
    "    print(\"-\"*60)\n",
    "    print(row['content'][:300] + \"...\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase4-header",
   "metadata": {},
   "source": [
    "## Phase 4: Update Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "update-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_articles_in_db(results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Update Supabase with scraped content\n",
    "    \n",
    "    Returns:\n",
    "        Dict with update statistics\n",
    "    \"\"\"\n",
    "    updated = 0\n",
    "    failed_updates = 0\n",
    "    \n",
    "    print(\"üîÑ Updating database...\\n\")\n",
    "    \n",
    "    for result in results:\n",
    "        if result['success']:\n",
    "            try:\n",
    "                # Update record\n",
    "                supabase.table('news_raw').update({\n",
    "                    'content_full': result['content'],\n",
    "                    'needs_full_scrape': False\n",
    "                }).eq('id', result['id']).execute()\n",
    "                \n",
    "                updated += 1\n",
    "                print(f\"‚úÖ Updated: {result['title'][:50]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_updates += 1\n",
    "                print(f\"‚ùå Failed to update {result['id']}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'updated': updated,\n",
    "        'failed': failed_updates\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Update function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-updates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update database with successful extractions\n",
    "print(f\"‚ö†Ô∏è  About to update {success_count} records in database\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "update_stats = update_articles_in_db(scraping_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä UPDATE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Successfully Updated: {update_stats['updated']}\")\n",
    "print(f\"Failed Updates: {update_stats['failed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-updates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify updates in database\n",
    "print(\"üîç Verifying database updates...\\n\")\n",
    "\n",
    "# Check total articles still needing scraping\n",
    "remaining = supabase.table('news_raw').select('*', count='exact').eq('needs_full_scrape', True).execute()\n",
    "\n",
    "# Check articles with content\n",
    "with_content = supabase.table('news_raw').select('*', count='exact').not_.is_('content_full', 'null').execute()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä DATABASE STATUS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Articles still needing scraping: {remaining.count}\")\n",
    "print(f\"Articles with full content: {with_content.count}\")\n",
    "print(f\"\\n‚úÖ Reduction: {82 - remaining.count} articles scraped from original 82\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase5-header",
   "metadata": {},
   "source": [
    "## Phase 5: Export Failed Articles & Full Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-failures",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export failed articles for investigation\n",
    "failed_df = df_results[df_results['success'] == False][['title', 'link', 'source', 'error']]\n",
    "\n",
    "if len(failed_df) > 0:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"../data/raw/failed_scrapes_{timestamp}.csv\"\n",
    "    failed_df.to_csv(filename, index=False)\n",
    "    print(f\"üìÅ Exported {len(failed_df)} failed articles to: {filename}\")\n",
    "else:\n",
    "    print(\"‚úÖ No failed articles to export!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-remaining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Process all remaining articles\n",
    "# ‚ö†Ô∏è  Uncomment and run this cell ONLY after validating test batch results\n",
    "\n",
    "# print(\"‚ö†Ô∏è  PROCESSING ALL REMAINING ARTICLES\")\n",
    "# print(f\"This will scrape ~{remaining.count} articles\")\n",
    "# print(f\"Estimated time: ~{remaining.count * 2.5 / 60:.1f} minutes\\n\")\n",
    "\n",
    "# # Fetch all remaining articles\n",
    "# remaining_articles = supabase.table('news_raw') \\\n",
    "#     .select('id, title, link, source, category') \\\n",
    "#     .eq('needs_full_scrape', True) \\\n",
    "#     .execute()\n",
    "\n",
    "# # Scrape in batches of 20\n",
    "# batch_size = 20\n",
    "# all_results = []\n",
    "\n",
    "# for i in range(0, len(remaining_articles.data), batch_size):\n",
    "#     batch = remaining_articles.data[i:i+batch_size]\n",
    "#     print(f\"\\nüîÑ Processing batch {i//batch_size + 1}...\")\n",
    "#     batch_results = scrape_articles_batch(batch)\n",
    "#     update_articles_in_db(batch_results)\n",
    "#     all_results.extend(batch_results)\n",
    "#     time.sleep(5)  # Pause between batches\n",
    "\n",
    "# print(\"\\n‚úÖ All articles processed!\")\n",
    "\n",
    "print(\"üí° Uncomment this cell to process all remaining articles after test validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-report",
   "metadata": {},
   "source": [
    "## Final Report & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "print(\"=\"*60)\n",
    "print(\"üìã FINAL SCRAPING REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest Batch Results:\")\n",
    "print(f\"  Attempted: {len(test_batch)}\")\n",
    "print(f\"  Successful: {success_count} ({success_rate:.1f}%)\")\n",
    "print(f\"  Failed: {failure_count}\")\n",
    "print(f\"  Avg Word Count: {successful_df['word_count'].mean():.0f} words\")\n",
    "\n",
    "print(f\"\\nDatabase Status:\")\n",
    "print(f\"  Total articles in DB: 150\")\n",
    "print(f\"  With full content: {with_content.count}\")\n",
    "print(f\"  Still need scraping: {remaining.count}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "if success_rate > 70:\n",
    "    print(\"  ‚úÖ Success rate is good! Process remaining articles.\")\n",
    "    print(\"  ‚úÖ Uncomment the 'process-remaining' cell to scrape all\")\n",
    "elif success_rate > 50:\n",
    "    print(\"  ‚ö†Ô∏è  Success rate is moderate. Review failed extractions.\")\n",
    "    print(\"  ‚ö†Ô∏è  Consider adjusting extraction parameters\")\n",
    "else:\n",
    "    print(\"  ‚ùå Success rate is low. May need custom scraper.\")\n",
    "    print(\"  ‚ùå Review The Hindu's HTML structure manually\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}